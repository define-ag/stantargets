---
title: "Pipelines with several MCMC runs"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Pipelines with several MCMC runs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_knit$set(root.dir = fs::dir_create(tempfile()))
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
if (identical(Sys.getenv("NOT_CRAN", unset = "false"), "false")) {
  knitr::opts_chunk$set(eval = FALSE)
}
library(cmdstanr)
library(dplyr)
library(targets)
library(stantargets)
```

It is sometimes desirable to simulate a single Bayesian model repeatedly across multiple simulated datasets. Examples:

1. Validate the implementation of a Bayesian model ([Cook, Gelman, and Rubin (2006)](http://www.stat.columbia.edu/~gelman/research/published/Cook_Software_Validation.pdf)).
2. Simulate a randomized controlled experiment to explore frequentist properties such as power and Type I error.

This vignette focuses on (1). The goal is to simulate multiple datasets from our model, analyze each dataset, and assess how often the estimated credible intervals for `beta` capture the true value of `beta` from the simulation.

```{r}
lines <- "data {
  int <lower = 1> n;
  vector[n] x;
  vector[n] y;
  real true_beta;
}
parameters {
  real beta;
}
model {
  y ~ normal(x * beta, 1);
  beta ~ normal(0, 1);
}"
writeLines(lines, "model.stan")
```

Next, we define a pipeline to simulate multiple datasets and fit each dataset with the model. Below, we commit to 10 replications: 2 batches with 5 iterations per batch. (In practical situations, the total number of replications should be hundreds of times more.) We also supply custom variable names and summary functions to return the 95% credible intervals for `beta`. Below, we use the `copy_data` argument to copy the true value of `beta` from the data to the results, which will help us assess the credible intervals.

```{r, output = FALSE}
library(targets)
tar_script({
  library(stantargets)
  options(crayon.enabled = FALSE)
  tar_option_set(memory = "transient", garbage_collection = TRUE)
  tar_pipeline(
    tar_stan_mcmc_rep_summary(
      model,
      "model.stan",
      tar_stan_example_data(),
      refresh = 0,
      init = 1,
      batches = 5, # Number of branch targets.
      reps = 2, # Number of model reps per branch target.
      show_messages = FALSE,
      copy_data = "true_beta", # Append scalars from data to the output data frame.
      variables = "beta",
      summaries = list(
        ~posterior::quantile2(.x, probs = c(0.025, 0.975))
      )
    )
  )
})
```

We now have a pipeline that runs the model 10 times: 5 batches (branch targets) with 2 replications per batch.

```{r}
tar_visnetwork(targets_only = TRUE)
```

Run the computation with `tar_make()`

```{r, output = FALSE, warning = FALSE}
tar_make()
```

The result is an aggregated data frame of summary statistics, where the `.rep` column distinguishes among individual replicates. We have the credible intervals for `beta` in columns `q2.5` and `q97.5`. And thanks to `copy_data`, we also have `true_beta`, the value of `beta` used to generate the dataset in each simulation rep.

```{r}
tar_load(model)
model
```

Now, let's assess how often the estimated 95% credible intervals capture the true values of `beta`. If the model is implemented correctly, the coverage value below should be close to 95%. (Ordinarily, we would [increase the number of batches and reps per batch](https://wlandau.github.io/targets-manual/dynamic.html#batching) and [run batches in parallel computing](https://wlandau.github.io/targets-manual/hpc.html).)

```{r}
library(dplyr)
model %>%
  summarize(coverage = mean(q2.5 < true_beta & true_beta < q97.5))
```
For maximum reproducibility, we should express the coverage assessment as a custom function and a target in the pipeline.

```{r, output = FALSE}
library(targets)
tar_script({
  library(stantargets)
  options(crayon.enabled = FALSE)
  tar_option_set(
    packages = "dplyr",
    memory = "transient",
    garbage_collection = TRUE
  )

  tar_pipeline(
    tar_stan_mcmc_rep_summary(
      model,
      "model.stan",
      tar_stan_example_data(),
      refresh = 0,
      init = 1,
      batches = 5,
      reps = 2,
      show_messages = FALSE,
      copy_data = "true_beta",
      variables = "beta",
      summaries = list(
        ~posterior::quantile2(.x, probs = c(0.025, 0.5, 0.975))
      )
    ),
    tar_target(
      coverage,
      model %>%
        summarize(coverage = mean(q2.5 < true_beta & true_beta < q97.5))
    )
  )
})
```

The new `coverage` target should the only outdated target, and it should be connected to the upstream `model` target.

```{r}
tar_visnetwork(targets_only = TRUE)
```

When we run the pipeline, only the coverage assessment should run. That way, we skip all the expensive computation of simulating datasets and running MCMC multiple times.

```{r, output = FALSE, warning = FALSE}
tar_make()
```

```{r}
tar_read(coverage)
```

## Multiple models

`tar_stan_rep_mcmc_summary()` and similar functions allow you to supply multiple Stan models. If you do, each model will share the the same collection of datasets. Below, we add a new `model2.stan` file to the `stan_files` argument of `tar_stan_rep_mcmc_summary()`. In the coverage summary below, we group by `.name` to compute a coverage statistic for each model.


```{r, output = FALSE}
library(targets)
tar_script({
  library(stantargets)
  options(crayon.enabled = FALSE)
  tar_option_set(
    packages = "dplyr",
    memory = "transient",
    garbage_collection = TRUE
  )

  tar_pipeline(
    tar_stan_mcmc_rep_summary(
      model,
      c("model.stan", "model2.stan"), # another model
      tar_stan_example_data(),
      refresh = 0,
      init = 1,
      batches = 5,
      reps = 2,
      show_messages = FALSE,
      copy_data = "true_beta",
      variables = "beta",
      summaries = list(
        ~posterior::quantile2(.x, probs = c(0.025, 0.5, 0.975))
      )
    ),
    tar_target(
      coverage,
      model %>%
        group_by(.name) %>%
        summarize(coverage = mean(q2.5 < true_beta & true_beta < q97.5))
    )
  )
})
```

In the graph below, notice how targets `model_model1` and `model_model2` are both connected to `model_data` upstream. Downstream, `model` is equivalent to `dplyr::bind_rows(model_model1, model_model2)`, and it will have special columns `.name` and `.file` to distinguish among all the models.

```{r}
tar_visnetwork(targets_only = TRUE)
```
