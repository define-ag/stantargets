---
title: "Pipelines with several MCMC runs"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Pipelines with several MCMC runs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_knit$set(root.dir = fs::dir_create(tempfile()))
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
if (identical(Sys.getenv("NOT_CRAN", unset = "false"), "false")) {
  knitr::opts_chunk$set(eval = FALSE)
}
library(cmdstanr)
library(dplyr)
library(targets)
library(stantargets)
```

It is sometimes desirable to simulate a single Bayesian model repeatedly across multiple simulated datasets. Examples:

1. Validate the implementation of a Bayesian model ([Cook, Gelman, and Rubin (2006)](http://www.stat.columbia.edu/~gelman/research/published/Cook_Software_Validation.pdf)).
2. Simulate a randomized controlled experiment to explore frequentist properties such as power and Type I error.

This vignette focuses on (1). The goal is to simulate multiple datasets from our model, analyze each dataset, and assess how often the estimated credible intervals for `beta` capture the true value of `beta` from the simulation. To make this easier, we will keep track of the true `beta` in a [transformed parameters](https://mc-stan.org/docs/2_18/reference-manual/program-block-transformed-parameters.html) block. We use the following model.

```{r}
lines <- "data {
  int <lower = 1> n;
  vector[n] x;
  vector[n] y;
  real true_beta;
}
parameters {
  real beta;
}
transformed parameters {
  real true_beta_value = true_beta;
}
model {
  y ~ normal(x * beta, 1);
  beta ~ normal(0, 1);
}"
writeLines(lines, "model.stan")
```

Next, we define a pipeline to simulate multiple datasets and fit each dataset with the model. Below, we commit to 10 replications: 2 batches with 5 iterations per batch. (In practical situations, the total number of replications should be hundreds of times more.) We also supply custom variable names and summary functions to return the 50% and 95% credible intervals for `beta`.

```{r, output = FALSE}
library(targets)
tar_script({
  library(stantargets)
  options(crayon.enabled = FALSE)
  tar_option_set(memory = "transient", garbage_collection = TRUE)
  tar_pipeline(
    tar_stan_mcmc_rep_summary(
      model,
      "model.stan",
      tar_stan_example_data(),
      refresh = 0,
      init = 1,
      batches = 5, # Number of branch targets.
      reps = 2, # Number of model reps per branch target.
      show_messages = FALSE,
      variables = c("beta", "true_beta_value"),
      summaries = list(
        ~posterior::quantile2(.x, probs = c(0.025, 0.5, 0.975))
      )
    )
  )
})
```

We now have a pipeline that runs the model 10 times: 5 batches (branch targets) with 2 replications per batch.

```{r}
tar_visnetwork(targets_only = TRUE)
```

Run the computation with `tar_make()`

```{r, output = FALSE, warning = FALSE}
tar_make()
```

Each batch returns a data frame of summary statistics, where the `.rep` column distinguishes among individual replicates.

```{r}
tar_read(model, branches = 1) # One batch
```

We can read all the branches at once into a larger data frame.

```{r}
tar_load(model)
model
```

Now, let's assess how often the estimated 95% credible intervals capture the true values of `beta`. If the model is implemented correctly, the coverage value below should be close to 95%. (Ordinarily, we would [increase the number of batches and reps per batch](https://wlandau.github.io/targets-manual/dynamic.html#batching) and [run batches in parallel computing](https://wlandau.github.io/targets-manual/hpc.html).)

```{r}
library(dplyr)
library(tidyr)
beta_ci <- model %>%
  filter(variable == "beta") %>%
  select(q2.5, q97.5, .rep)
beta_truth <- model %>%
  filter(variable == "true_beta_value") %>%
  rename(truth = q50) %>%
  select(truth, .rep)
left_join(beta_ci, beta_truth, by = ".rep") %>%
  summarize(coverage = mean(q2.5 < truth & truth < q97.5))
```
For maximum reproducibility, we should express the coverage assessment as a custom function and a target in the pipeline.

```{r, output = FALSE}
library(targets)
tar_script({
  library(stantargets)
  options(crayon.enabled = FALSE)
  tar_option_set(
    packages = c("dplyr", "tidyr"),
    memory = "transient",
    garbage_collection = TRUE
  )

  assess_coverage <- function(results) {
    beta_ci <- results %>%
      filter(variable == "beta") %>%
      select(q2.5, q97.5, .rep)
    beta_truth <- results %>%
      filter(variable == "true_beta_value") %>%
      rename(truth = q50) %>%
      select(truth, .rep)
    left_join(beta_ci, beta_truth, by = ".rep") %>%
      summarize(coverage = mean(q2.5 < truth & truth < q97.5))
  }

  tar_pipeline(
    tar_stan_mcmc_rep_summary(
      model,
      "model.stan",
      tar_stan_example_data(),
      refresh = 0,
      init = 1,
      batches = 5,
      reps = 2,
      show_messages = FALSE,
      variables = c("beta", "true_beta_value"),
      summaries = list(
        ~posterior::quantile2(.x, probs = c(0.025, 0.5, 0.975))
      )
    ),
    tar_target(coverage, assess_coverage(model))
  )
})
```

The new `coverage` target should the only outdated target, and it should be connected to the upstream `model` target.

```{r}
tar_visnetwork(targets_only = TRUE)
```

When we run the pipeline, only the coverage assessment should run. That way, we skip all the expensive computation of simulating datasets and running MCMC multiple times.

```{r, output = FALSE, warning = FALSE}
tar_make()
```

```{r}
tar_read(coverage)
```
